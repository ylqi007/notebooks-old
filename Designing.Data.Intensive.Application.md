[TOC]

# Part I. Foundations of Data Systems

# Chapter 01 Reliable, Scalable, and Maintainable Applications (可靠性、可扩展性、可维护性)
* 数据密集型应用(data-intensive): 现如今很多应用是data-intensive，因此CPU很少成为这类应用的瓶颈。更大的问题来自**数据量、数据复杂性、以及数据的变更速度**。
* 计算密集型应用(compute-intensive)

**数据密集型应用**通常由标准件构建而成，标准组件提供了很多通用的功能。例如，许多应用都需要
* 数据库: Store data so that they, or another application, can find it again later (databases)
* 缓存(caches): Remember the result of an expensive operation, to speed up reads (caches)
* 搜索索引(search indexes): Allow users to search data by keyword or filter it in various ways (search indexes)
* 流处理(stream processing): Send a message to another process, to be handled asynchronously (stream processing)
* 批处理(batch processing): Perioridally crunch a large amount of accumulated data (batch processing)
上述功能听起来平平无奇，那是因为如箭的**数据系统(data system)**已经是非常成功的抽象了。并且绝大多数工程师不会幻想从零开始编写存储引擎，因为在开发应用时，数据库已经是足够完美的工具了。

单现实并没有那么简单。不同的应用有不同的需求，因而数据库系统也是百花齐放，有各式各样的特性。实现缓存有很多手段，创建搜索索引也有好几种方法，等等。因此，在开发应用前，我们有必要先弄清楚嘴时候手头工作的工具和方法。而且，当单个工具解决不了你的问题时，组合使用这些工具可能还是有些难度的。

## 1.1 Thinking About Data Systems
我们通常认为，数据库、消息队列、缓存等工具分属几个差异显著的类别。但是由于以下各种原因，我们把这些概念都放在数据系统(data system)的总称之下
1. 近些年来，出现了许多新的**数据存储工具**与**数据处理工具**。它们针对不同应用场景进行优化，因此不再适合生硬地归入传统类别。因此，类别之间的界限变得越来越模糊。
2. 越来越多的应用程序有各种严格而广泛的要求，单个工具不足以满足所有的数据处理和存储需求。取而代之的是，总体工作被拆分成一些列能被单个工具高效完成的任务，并通过应用代码将它们组合起来。

当你将多个工具组合在一起提供服务时，服务的借口或应用程序编程接口(API, Application Programming Interface)通常向客户端隐藏这些实现细节。

在设计大多数软件系统时，有三个很重要的问题：
1. 可靠性(Reliability): 系统在困境(硬件故障、软件故障、人为错误)中仍然可以正常工作(正确完成功能，并能达到期望的性能水平)。
2. 可扩展性(Scalability): 有合理的办法应对系统的增长(数据量、流量、复杂性)。
3. 可维护性(Maintainability): 许多不同的人(工程师、运维)在不同的生命周期，都能够高效地在系统上工作(使系统保持现有行为，并适应新的应用场景)。

## 1.2 Reliability
造成错误的原因叫做故障(fault)，能预料并应对故障的系统特性可称为**容错(fault-tolerant)**或**韧性(resilient)**。容错并不意味着可以容忍**所有**可能的错误。

❗️注意：故障(fault)不同于失效(failure)。**故障**通常定义为系统的一部分状态偏离其标准，而失效则是系统作为一个整体停止向用户提供服务。故障的概率不可能降到零，因此最好设计容错机制以防止故障而导致失效。

在容错系统中，通常故意触发来提高故障率是有意义的。我们可以通过故意引发故障来确保容错机制不同运行并接受考验，从而提高故障自然发生时系统能够正确处理的信心。

尽管比起**阻止错误(prevent error)**，我们通常更倾向于容忍错误。

### 1.2.1 Hardware Faults
当想到系统失效的原因时，**硬件故障(hardware faults)**总会第一个进入脑海。硬盘崩溃、内存出错、机房断电、有人拔错网线......任何与大型数据中心打过交道的人都会告诉你：一旦你拥有很多机器，这些事情总会发生！

为了减少系统的故障率，第一反应通常都是增加单个硬件的**冗余度**。这种方法虽然不能完全防止由硬件问题导致的系统失效，但它简单易懂，通常也足以让机器不间断运行很多年。

直到最近，硬件冗余对于大多数应用来说已经足够了，它使单台机器完全失效变得相当罕见。

但是随着数据量和应用计算需求的增加，越来越做的应用开始大量使用机器，这会相应地增加硬件故障率。

如果在**硬件冗余**的基础上进一步引入软件容错机制，那么系统在容忍整个(单台)机器故障的道路上就更进一步了。

### 1.2.2 Software Errors
我们通常认为硬件故障是随机的、相互独立的：一台机器的磁盘失效并不意味着另一台机器的磁盘也会失效。大量硬件组件不可能同时发生故障，除非它们存在比较弱的相关性(同样的原因导致关联性错误，例如，服务器机架的温度)。

另一类错误是内部的**系统性错误(systematic error)**。这类错误难以预料，而且因为是跨节点相关的，所以比起不相关的硬件故障，往往可能造成更多的系统失效。

导致类型软件的故障的BUG通常会潜伏很长时间，直到被异常情况触发为止。这种情况意味着软件对其环境做出了某种假设--虽然这种假设通常来说是正确的，但由于某种原因最后不再成立。

### 1.2.3 Human Errors
设计并构建软件系统的工程师是人类，维持系统运行的运维也是人类。即使它们怀有最大的善意，人类也是不可靠的。

尽管人类不可靠，但是怎么做才会让系统变得可靠？最好的系统会组合使用以下方法：
* 以最小化犯错误的方式设计系统。例如，精心设计的抽象、API和管理后台使做对的事情变得更容易，搞砸事情变得困难。但如果接口限制太多，人们就会忽略它们的好处而想办法绕开。很难正确把握这种微妙的平衡。
* 将人们最容易犯错的地方与可能导致失效的地方**解耦(decouple)**。特别是提供一个功能齐全的非生产环境**沙箱(sandbox)**，使人们可以在不影响真实用户的情况下，使用真实数据安全地探索和实验。
* 在各个层次进行彻底的测试，从单元测试、全系统集成测试到手动测试。自动化测试易于理解，已经被广泛使用，特别适合用来覆盖正常情况中少见的**边缘场景(corner case)**。
* 允许从人为错误中简单快速恢复，以最大限度地减少失效情况带来的影响。例如，快速回滚配置变更，分批发布新代码(以便任何意外错误只影响一小部分用户)，并提供数据重算工具(以备旧的计算出错)。
* 配置详细和明确的监控，比如性能指标和错误率。 在其他工程学科中这指的是遥测（telemetry）。 （一旦火箭离开了地面，遥测技术对于跟踪发生的事情和理解失败是至关重要的。）监控可以向我们发出预警信号，并允许我们检查是否有任何地方违反了假设和约束。当出现问题时，指标数据对于问题诊断是非常宝贵的。
* 良好的管理实践与充分的培训——一个复杂而重要的方面，但超出了本书的范围。

### 1.2.4 How Important is Reliability?
​ 可靠性不仅仅是针对核电站和空中交通管制软件而言，我们也期望更多平凡的应用能可靠地运行。商务应用中的错误会导致生产力损失（也许数据报告不完整还会有法律风险），而电商网站的中断则可能会导致收入和声誉的巨大损失。

即使在“非关键”应用中，我们也对用户负有责任。试想一位家长把所有的照片和孩子的视频储存在你的照片应用里。如果数据库突然损坏，他们会感觉如何？他们可能会知道如何从备份恢复吗？

在某些情况下，我们可能会选择牺牲可靠性来降低开发成本（例如为未经证实的市场开发产品原型）或运营成本（例如利润率极低的服务），但我们偷工减料时，应该清楚意识到自己在做什么。

## 1.3 Scalability
系统今天能可靠运行，并不意味未来也能可靠运行。服务 降级（degradation） 的一个常见原因是负载增加，例如：系统负载已经从一万个并发用户增长到十万个并发用户，或者从一百万增长到一千万。也许现在处理的数据量级要比过去大得多。

可扩展性（Scalability） 是用来描述系统应对负载增长能力的术语

### 1.3.1 Describing Load
在讨论增长问题（如果负载加倍会发生什么？）前，首先要能简要描述系统的当前负载。负载可以用一些称为 负载参数（load parameters） 的数字来描述。参数的最佳选择取决于系统架构，它可能是每秒向Web服务器发出的请求、数据库中的读写比率、聊天室中同时活跃的用户数量、缓存命中率或其他东西。除此之外，也许平均情况对你很重要，也许你的瓶颈是少数极端场景。

### 1.3.2 Describing Performance
一旦系统的负载被描述好，就可以研究当负载增加会发生什么。我们可以从两种角度来看：
* 增加负载参数并保持系统资源（CPU、内存、网络带宽等）不变时，系统性能将受到什么影响？
* 增加负载参数并希望保持性能不变时，需要增加多少系统资源？

这两个问题都需要性能数据，所以让我们简单地看一下如何描述系统性能。

通常报表都会展示服务的平均响应时间。 

通常使用**百分位点(percentiles)**会更好。如果将响应时间列表按最快到最慢排序，那么**中位数(median)**就在正中间：举个例子，如果你的响应时间中位数是200毫秒，这意味着一半请求的返回时间少于200毫秒，另一半比这个要长。

如果想知道典型场景下用户需要等待多长时间，那么中位数是一个好的度量标准：一半用户请求的响应时间少于响应时间的中位数，另一半服务时间比中位数长。

响应时间的高百分位点（也称为尾部延迟（tail latencies））非常重要，因为它们直接影响用户的服务体验。例如亚马逊在描述内部服务的响应时间要求时以99.9百分位点为准，即使它只影响一千个请求中的一个。这是因为请求响应最慢的客户往往也是数据最多的客户，也可以说是最有价值的客户 —— 因为他们掏钱了.

 百分位点通常用于服务级别目标（SLO, service level objectives）和服务级别协议（SLA, service level agreements），即定义服务预期性能和可用性的合同。 SLA可能会声明，如果服务响应时间的中位数小于200毫秒，且99.9百分位点低于1秒，则认为服务工作正常（如果响应时间更长，就认为服务不达标）。这些指标为客户设定了期望值，并允许客户在SLA未达标的情况下要求退款。

 排队延迟（queueing delay） 通常占了高百分位点处响应时间的很大一部分。由于服务器只能并行处理少量的事务（如受其CPU核数的限制），所以只要有少量缓慢的请求就能阻碍后续请求的处理，这种效应有时被称为 头部阻塞（head-of-line blocking） 。即使后续请求在服务器上处理的非常迅速，由于需要等待先前请求完成，客户端最终看到的是缓慢的总体响应时间。因为存在这种效应，测量客户端的响应时间非常重要。

### 1.3.3 Approaches for Coping with Load
在已经讨论了描述负载的参数和用于衡量性能的指标。现在可以认真讨论可扩展性了：当负载参数增加时，如何保持良好的性能？

适应某个级别负载的架构不太可能应付10倍于此的负载。如果你正在开发一个快速增长的服务，那么每次负载发生数量级的增加时，你都可能需要重新考虑构架--或者更频繁。

人们经常讨**论纵向扩展(scaling up)**(垂直扩展(vertical scaling)，转向更强大的机器)和**横向扩展(scaling out)**(水平扩展(horizontal scaling)，将负载分布到多台小机器上)之间的对立。跨多台机器分配负载也称为“无共享（shared-nothing）”架构。可以在单台机器上运行的系统通常更简单，但高端机器可能非常贵，所以非常密集的负载通常无法避免地需要横向扩展。现实世界中的优秀架构需要将这两种方法务实地结合，因为使用几台足够强大的机器可能比使用大量的小型虚拟机更简单也更便宜。
* 纵向扩展：转向更强大的机器
* 横向扩展：将负载分布到多台小机器上

有些系统是 弹性（elastic） 的，这意味着可以在检测到负载增加时自动增加计算资源，而其他系统则是手动扩展（人工分析容量并决定向系统添加更多的机器）。如果负载极难预测（highly unpredictable），则弹性系统可能很有用，但手动扩展系统更简单，并且意外操作可能会更少。

跨多台机器部署无状态服务（stateless services）非常简单，但将带状态的数据系统从单节点变为分布式配置则可能引入许多额外复杂度。出于这个原因，常识告诉我们应该将数据库放在单个节点上（纵向扩展），直到扩展成本或可用性需求迫使其改为分布式。

## 1.4 Maintainability
众所周知，软件的大部分开销并不在最初的开发阶段，而是在持续的维护阶段，包括修复漏洞、保持系统正常运行、调查失效、适配新的平台、为新的场景进行修改、偿还技术债、添加新的功能等等。

但是我们可以，也应该以这样一种方式来设计软件：在设计之初就尽量考虑可能减少维护期间的痛苦，从而避免自己的软件变成遗留系统。为此，我们将特别关注软件系统的三个原则：
* 可操作性(Operability): 便于运维团队保持系统平稳运行。 
* 简单性(Simplicity): 从系统中消除尽可能多的复杂度(complexity)，使新工程师也能轻松理解系统。
* 可演化性(Evolability): 使工程师在未来能够轻松地对系统进行更改，当需求变化时，为新应用场景做适配。也称为*可扩展性(extensibility),可修改性(modifiability),或可塑形(plasticity)**。

### 1.4.1 Operability: Making Life Easy for Operations

### 1.4.2 Simplicity: Managing Complexity
**复杂度(complexity)**有各种可能的症状，例如：状态空间激增、模块间紧密耦合、纠结的依赖关系、不一致的命名和术语、解决性能问题的Hack、需要绕开的特例等等，现在已经有很多关于这个话题的讨论。

因为复杂度导致维护困难时，预算和时间安排通常会超支。在复杂的软件中进行变更，引入错误的风险也更大：当开发人员难以理解系统时，隐藏的假设、无意的后果和意外的交互就更容易被忽略。相反，降低复杂度能极大地提高软件的可维护性，因此简单性应该是构建系统的一个关键目标。

简化系统并不一定意味着减少功能：它也可以意味着消**除额外(accidental)**的负载度。Moseley和Marks把**额外复杂度**定义为：由具体实现中涌现，而非问题本身固有的复杂度。

用于消除**额外复杂度**最好的工具之一是**抽象(abstraction)**。一个好的抽象可以将大量实现细节隐藏在一个干净，简单易懂的外观下面。一个好的抽象也也可以广泛用于各类不同应用。比起重复造很多轮子，重用抽象不仅仅更有效率，而且有助于开发高质量的软件。抽象组件的质量改进将使所有使用它的应用收益。

### 1.4.3 Evolvability: Making Change Easy
系统的需求永远不变，基本是不可能的。更可能的情况是，它们处于常态的变化中，例如：你了解了新的事实、出现意想不到的应用场景、业务优先级发生变化、用户要求新功能、新平台取代旧平台、法律或监管要求发生变化、系统增长迫使架构变化等。

在组织流程方面，敏捷(agile)工作模式为适应变化提供了一个框架。敏捷社区还开发了对在频繁变化的环境中开发软件很有帮助的技术工具和模式，如**测试驱动开发(TDD, test-driven development)**和**重构(refactoring)**。

修改数据系统并使其适应不断变化需求的容易程度，是与**简单性**和**抽象性**密切相关的：简单易懂的系统通常比复杂系统更容易修改。但由于这是一个非常重要的概念，我们将用一个不同的词来指代数据系统层面的敏捷性：可演化性(evolvability)。

## 1.5 Summary
一个应用必须满足各种需求才称得上有用。有一些功能需求（functional requirements）（它应该做什么，比如允许以各种方式存储，检索，搜索和处理数据）以及一些非功能性需求（nonfunctional）（通用属性，例如安全性，可靠性，合规性，可扩展性，兼容性和可维护性）。在本章详细讨论了可靠性，可扩展性和可维护性。
* 可靠性（Reliability） 意味着即使发生故障，系统也能正常工作。故障可能发生在硬件（通常是随机的和不相关的），软件（通常是系统性的Bug，很难处理），和人类（不可避免地时不时出错）。 容错技术 可以对终端用户隐藏某些类型的故障。
* 可扩展性（Scalability） 意味着即使在负载增加的情况下也有保持性能的策略。为了讨论可扩展性，我们首先需要定量描述负载和性能的方法。我们简要了解了推特主页时间线的例子，介绍描述负载的方法，并将响应时间百分位点作为衡量性能的一种方式。在可扩展的系统中可以添加 处理容量（processing capacity） 以在高负载下保持可靠。
* 可维护性（Maintainability） 有许多方面，但实质上是关于工程师和运维团队的生活质量的。良好的抽象可以帮助降低复杂度，并使系统易于修改和适应新的应用场景。良好的可操作性意味着对系统的健康状态具有良好的可见性，并拥有有效的管理手段。

不幸的是，使应用可靠、可扩展或可维护并不容易。但是某些模式和技术会不断重新出现在不同的应用中。在接下来的几章中，我们将看到一些数据系统的例子，并分析它们如何实现这些目标。